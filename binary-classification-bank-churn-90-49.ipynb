{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"},{"sourceId":3322096,"sourceType":"datasetVersion","datasetId":2008274}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color: pink; font-size: 40px; text-align: center;\"> Detailed Explanation of Binary Classification Bank Churn Project with Prediction: 90.49% </div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: orange; font-size: 30px; text-align: center;\"> Datasets\n    </div>\n    \n1. https://www.kaggle.com/competitions/playground-series-s4e1\n2. https://www.kaggle.com/datasets/shantanudhakadd/bank-customer-churn-prediction","metadata":{}},{"cell_type":"markdown","source":"# **Part 1: Libraries and Settings**\n\n#### **Libraries:**\n- **NumPy (`np`):** NumPy is a powerful library for numerical operations in Python. It provides support for large, multi-dimensional arrays and matrices, along with mathematical functions to operate on these elements.\n\n- **Pandas (`pd`):** Pandas is a data manipulation library that provides data structures like DataFrames for easy handling and analysis of structured data.\n\n- **Matplotlib (`plt`):** Matplotlib is a 2D plotting library for creating static, animated, and interactive visualizations in Python.\n\n- **Seaborn (`sns`):** Seaborn is a statistical data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n\n- **TensorFlow (`tf`):** TensorFlow is an open-source machine learning framework developed by Google. It is widely used for building and training machine learning models, particularly deep learning models.\n\n- **Optuna:** Optuna is an optimization framework for hyperparameter tuning. It simplifies the optimization of machine learning model hyperparameters.\n\n- **Category Encoders:** A library for encoding categorical variables. Different encoding methods like One-Hot, M-Estimate, CatBoost, and Ordinal encoding are available.\n\n- **Scikit-learn:** A comprehensive machine learning library that includes various tools for data preprocessing, model selection, evaluation, and more.\n\n- **XGBoost, LightGBM, CatBoost:** These are gradient boosting libraries widely used for classification and regression tasks. They provide efficient implementations of boosting algorithms.\n\n#### **Settings:**\n- **Seaborn Theme and Palette:** Sets the theme and color palette for Seaborn plots to ensure consistent and visually appealing visualizations.\n\n- **Pandas Settings:** Configures various display options for Pandas DataFrames, such as maximum rows, columns, and float formatting.\n\n- **File Reading:** Reads the training and test datasets from CSV files using Pandas. The `index_col` parameter specifies the column to use as the index.\n\nLet's move on to the next part. If you have any questions or if you'd like more details on any specific aspect, feel free to ask!","metadata":{}},{"cell_type":"markdown","source":"# **Part 2: Data Loading and Exploration**\n\n#### **Data Exploration - Training Dataset (`train`):**\n```python\ntrain.head(10)\n```\n- Displays the first 10 rows of the training dataset for initial inspection.\n\n```python\ndesc = pd.DataFrame(index=list(train))\ndesc['type'] = train.dtypes\ndesc['count'] = train.count()\ndesc['nunique'] = train.nunique()\ndesc['%unique'] = desc['nunique'] / len(train) * 100\ndesc['null'] = train.isnull().sum()\ndesc['%null'] = desc['null'] / len(train) * 100\ndesc['min'] = train.min()\ndesc['max'] = train.max()\ndesc\n```\n- Creates a descriptive DataFrame (`desc`) containing information about each column in the training dataset.\n  - `type`: Data type of each column.\n  - `count`: Non-null count of values.\n  - `nunique`: Number of unique values.\n  - `%unique`: Percentage of unique values.\n  - `null`: Number of null values.\n  - `%null`: Percentage of null values.\n  - `min`: Minimum value.\n  - `max`: Maximum value.\n\n- Provides insights into the structure and characteristics of the training dataset, including the presence of null values, data types, and descriptive statistics.\n\n#### **Data Exploration - Test Dataset (`test`):**\n```python\ntest.head(10)\n```\n- Displays the first 10 rows of the test dataset for initial inspection.\n\n```python\ndesc = pd.DataFrame(index=list(test))\ndesc['type'] = test.dtypes\ndesc['count'] = test.count()\ndesc['nunique'] = test.nunique()\ndesc['%unique'] = desc['nunique'] / len(test) * 100\ndesc['null'] = test.isnull().sum()\ndesc['%null'] = desc['null'] / len(test) * 100\ndesc['min'] = test.min()\ndesc['max'] = test.max()\ndesc\n```\n- Creates a descriptive DataFrame (`desc`) for the test dataset, similar to what was done for the training dataset.\n\n- Provides insights into the structure and characteristics of the test dataset.\n\n#### **Data Exploration - Original Dataset (`orig_train`):**\n```python\norig_train.head(10)\n```\n- Displays the first 10 rows of the original training dataset (`Churn_Modelling.csv`) for initial inspection.\n\n```python\ndesc = pd.DataFrame(index=list(orig_train))\ndesc['type'] = orig_train.dtypes\ndesc['count'] = orig_train.count()\ndesc['nunique'] = orig_train.nunique()\ndesc['%unique'] = desc['nunique'] / len(orig_train) * 100\ndesc['null'] = orig_train.isnull().sum()\ndesc['%null'] = desc['null'] / len(orig_train) * 100\ndesc['min'] = orig_train.min()\ndesc['max'] = orig_train.max()\ndesc\n```\n- Creates a descriptive DataFrame (`desc`) for the original training dataset, similar to what was done for the other datasets.\n\n- Provides insights into the structure and characteristics of the original training dataset.\n\n#### **Feature Categorization:**\n```python\nnumerical_features = list(test._get_numeric_data())\ncategorical_features = list(test.drop(numerical_features, axis=1))\n```\n- Separates features into numerical and categorical types based on the test dataset.\n\nThese steps aim to understand the data, identify potential issues (such as missing values), and categorize features for further analysis and preprocessing. If you have any specific questions or if you'd like more details on any part, feel free to ask!","metadata":{}},{"cell_type":"markdown","source":"# **Part 3: Data Preparation**\n\n#### **Combining Datasets:**\n```python\nX = pd.concat([orig_train, train]).reset_index(drop=True)\n```\n- Concatenates the original training dataset (`orig_train`) and the training dataset (`train`) along rows (`axis=0`).\n- Resets the index to create a new combined dataset (`X`).\n\n```python\ny = X.pop('Exited')\n```\n- Extracts the target variable (`Exited`) from the combined dataset (`X`) and assigns it to `y`.\n- The target variable is removed from `X` to create the feature matrix.\n\n```python\norig_comp_combo = train.merge(orig_train, on=list(test), how='left')\norig_comp_combo.index = train.index\n\norig_test_combo = test.merge(orig_train, on=list(test), how='left')\norig_test_combo.index = test.index\n```\n- Merges the training dataset (`train`) with the original training dataset (`orig_train`) based on the common columns present in the test dataset.\n- The result is stored in `orig_comp_combo`, and the index is set to match the index of the training dataset.\n\n- Similar steps are performed for the test dataset, resulting in `orig_test_combo`.\n\n#### **Cross-Validation Setup:**\n```python\nseed = 42\nsplits = 30\nskf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n```\n- Sets the random seed for reproducibility (`seed`).\n- Configures the Stratified K-Folds cross-validator (`skf`) with 30 splits and shuffling, which is commonly used for classification tasks.\n- The stratified nature ensures that each fold maintains the same distribution of target classes as the original dataset.\n\n#### **TensorFlow and Random Seed Configuration:**\n```python\ntf.keras.utils.set_random_seed(seed)\ntf.config.experimental.enable_op_determinism()\n```\n- Sets the random seed for TensorFlow (`set_random_seed`) to ensure reproducibility.\n- Enables deterministic operations in TensorFlow (`enable_op_determinism`), further contributing to reproducibility.\n\nThis part focuses on preparing the data for machine learning modeling, combining datasets, extracting target variables, and configuring settings for cross-validation and random seeds to ensure consistent and reproducible results during model training. If you have any specific questions or if you'd like more details on any part, feel free to ask!","metadata":{}},{"cell_type":"markdown","source":"# **Part 4: Feature Engineering**\n\n#### **Custom Transformer Functions:**\n\n1. **Nullify:**\n   ```python\n   def nullify(x):\n       x_copy = x.copy()\n       x_copy['Balance'] = x_copy['Balance'].replace({0: np.nan})\n       return x_copy\n\n   Nullify = FunctionTransformer(nullify)\n   ```\n   - Replaces zero balances in the 'Balance' column with NaN, utilizing the `replace` method.\n   - Implemented as a custom transformer using `FunctionTransformer`.\n\n2. **SalaryRounder:**\n   ```python\n   def salary_rounder(x):\n       x_copy = x.copy()\n       x_copy['EstimatedSalary'] = (x_copy['EstimatedSalary'] * 100).astype(np.uint64)\n       return x_copy\n\n   SalaryRounder = FunctionTransformer(salary_rounder)\n   ```\n   - Multiplies the 'EstimatedSalary' column by 100 and converts it to uint64.\n   - Another custom transformer using `FunctionTransformer`.\n\n3. **AgeRounder:**\n   ```python\n   def age_rounder(x):\n       x_copy = x.copy()\n       x_copy['Age'] = (x_copy['Age'] * 10).astype(np.uint16)\n       return x_copy\n\n   AgeRounder = FunctionTransformer(age_rounder)\n   ```\n   - Multiplies the 'Age' column by 10 and converts it to uint16.\n   - A custom transformer similar to the previous ones.\n\n4. **BalanceRounder:**\n   ```python\n   def balance_rounder(x):\n       x_copy = x.copy()\n       x_copy['Balance'] = (x_copy['Balance'] * 100).astype(np.uint64)\n       return x_copy\n\n   BalanceRounder = FunctionTransformer(balance_rounder)\n   ```\n   - Multiplies the 'Balance' column by 100 and converts it to uint64.\n   - Implemented as a custom transformer using `FunctionTransformer`.\n\n5. **FeatureGenerator:**\n   ```python\n   def feature_generator(x):\n       x_copy = x.copy()\n       # x_copy['IsSenior'] = (x_copy['Age'] >= 600).astype(np.uint8)\n       x_copy['IsActive_by_CreditCard'] = x_copy['HasCrCard'] * x_copy['IsActiveMember']\n       x_copy['Products_Per_Tenure'] = x_copy['Tenure'] / x_copy['NumOfProducts']\n       x_copy['ZeroBalance'] = (x_copy['Balance'] == 0).astype(np.uint8)\n       x_copy['AgeCat'] = np.round(x_copy.Age / 20).astype(np.uint16)\n       x_copy['AllCat'] = x_copy['Surname'] + x_copy['Geography'] + x_copy['Gender'] + x_copy.EstimatedSalary.astype(\n           'str') + x_copy.CreditScore.astype('str') + x_copy.Age.astype('str') + x_copy.NumOfProducts.astype(\n           'str') + x_copy.Tenure.astype('str') + x_copy.CustomerId.astype('str')\n\n       return x_copy\n\n   FeatureGenerator = FunctionTransformer(feature_generator)\n   ```\n   - Generates new features based on existing columns.\n   - Creates features such as 'IsActive_by_CreditCard', 'Products_Per_Tenure', 'ZeroBalance', 'AgeCat', and 'AllCat'.\n   - Implemented as a custom transformer using `FunctionTransformer`.\n\n6. **SVDRounder:**\n   ```python\n   def svd_rounder(x):\n       x_copy = x.copy()\n       for col in [column for column in list(x) if 'SVD' in column]:\n           x_copy[col] = (x_copy[col] * 1e18).astype(np.int64)\n\n       return x_copy\n\n   SVDRounder = FunctionTransformer(svd_rounder)\n   ```\n   - Rounds the values in columns containing 'SVD' in their names by multiplying them with 1e18 and converting to int64.\n   - Custom transformer created with `FunctionTransformer`.\n\nThese custom transformers are designed to preprocess and engineer features in the dataset. They encapsulate specific operations, making it easier to apply them consistently and modularly within the data preparation pipeline. If you have any questions or need further clarification, feel free to ask!","metadata":{}},{"cell_type":"markdown","source":"> 1. **IsActive_by_CreditCard:**\n>    - **Logic:** Multiplying 'HasCrCard' (whether the customer has a credit card) by 'IsActiveMember' (whether the customer is an active member).\n>    - **Purpose:** This feature captures the interaction between having a credit card and being an active member, potentially revealing patterns specific to this combination.\n> \n> 2. **Products_Per_Tenure:**\n>    - **Logic:** Calculating the ratio of 'Tenure' (the number of years the customer has been with the bank) to 'NumOfProducts' (the number of bank products the customer uses).\n>    - **Purpose:** This feature normalizes the tenure by the number of products, providing a perspective on the average duration a customer holds a product.\n> \n> 3. **ZeroBalance:**\n>    - **Logic:** Creating a binary indicator for whether the 'Balance' is zero.\n>    - **Purpose:** This binary feature identifies customers with zero balances, which might have distinct behaviors or characteristics.\n> \n> 4. **AgeCat:**\n>    - **Logic:** Dividing 'Age' by 20 and rounding to assign the customer to an age category.\n>    - **Purpose:** This discretizes the age into categories, potentially capturing non-linear relationships with the target variable.\n> \n> 5. **AllCat:**\n>    - **Logic:** Concatenating several categorical and numerical columns after converting them to strings.\n>    - **Purpose:** This feature combines information from multiple columns into a single categorical feature, potentially capturing complex patterns and interactions.\n> \n> These feature engineering techniques aim to uncover hidden patterns, relationships, or variations in the data that might be valuable for predictive modeling. It's an iterative process where the creation of features is guided by domain knowledge, intuition, and experimentation. If you have further questions or need clarification on any specific aspect, feel free to ask!","metadata":{}},{"cell_type":"markdown","source":"# **Part 5:Feature Engineering:**\n\n#### 5.1 FeatureDropper:\n```python\nclass FeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, cols):\n        self.cols = cols\n\n    def fit(self, x, y):\n        return self\n\n    def transform(self, x):\n        return x.drop(self.cols, axis=1)\n```\n- **Purpose:**\n  - **Logic:** Drops specified columns from the input DataFrame `x`.\n  - **Purpose:** This class provides a transformer to drop unnecessary or unwanted columns during the data preprocessing pipeline.\n\n#### 5.2 Categorizer:\n```python\nclass Categorizer(BaseEstimator, TransformerMixin):\n    def __init__(self, cols: list):\n        self.cols = cols\n\n    def fit(self, x, y):\n        return self\n\n    def transform(self, x):\n        return x.astype({cat: 'category' for cat in self.cols})\n```\n- **Purpose:**\n  - **Logic:** Converts specified columns to the 'category' data type.\n  - **Purpose:** This transformer is useful when dealing with categorical variables in machine learning models that benefit from the 'category' dtype, such as LightGBM or CatBoost.\n\n#### 5.3 Vectorizer:\n```python\nclass Vectorizer(BaseEstimator, TransformerMixin):\n    def __init__(self, max_features=1000, cols=['Surname'], n_components=3):\n        self.max_features = max_features\n        self.cols = cols\n        self.n_components = n_components\n\n    def fit(self, x, y):\n        self.vectorizer_dict = {}\n        self.decomposer_dict = {}\n\n        for col in self.cols:\n            self.vectorizer_dict[col] = TfidfVectorizer(max_features=self.max_features).fit(x[col].astype(str), y)\n            self.decomposer_dict[col] = TruncatedSVD(random_state=seed, n_components=self.n_components).fit(\n                self.vectorizer_dict[col].transform(x[col].astype(str)), y\n            )\n\n        return self\n\n    def transform(self, x):\n        vectorized = {}\n\n        for col in self.cols:\n            vectorized[col] = self.vectorizer_dict[col].transform(x[col].astype(str))\n            vectorized[col] = self.decomposer_dict[col].transform(vectorized[col])\n\n        vectorized_df = pd.concat([pd.DataFrame(vectorized[col]).rename({\n            f'truncatedsvd{i}': f'{col}SVD{i}' for i in range(self.n_components)\n        }, axis=1) for col in self.cols], axis=1)\n\n        return pd.concat([x.reset_index(drop=True), vectorized_df], axis=1)\n```\n- **Purpose:**\n  - **Logic:**\n    - Fits a TfidfVectorizer and TruncatedSVD for each specified column.\n    - Transforms the original columns into a reduced-dimensional space using TruncatedSVD.\n  - **Purpose:** This class is designed for columns containing textual data (like 'Surname'). It converts the text into numerical features using TF-IDF and reduces dimensionality with TruncatedSVD, which can be useful for machine learning models. This is especially common in Natural Language Processing (NLP) tasks.","metadata":{}},{"cell_type":"markdown","source":"# **Part 6: Model Cross-Validation:**\nCertainly! Let's break down the `cross_val_score` function step by step:\n\n#### 6.1 `cross_val_score` Function:\n\n```python\ndef cross_val_score(estimator, cv=skf, label='', include_original=True, show_importance=False, add_reverse=False):\n    X = train.copy()\n    y = X.pop('Exited')\n```\n- **Explanation:**\n  - `cross_val_score` is a function that performs cross-validation for a given machine learning model (`estimator`).\n  - `cv=skf` sets the default cross-validation strategy to Stratified K-Fold (`skf` is defined earlier).\n  - `label=''` is a parameter for labeling purposes.\n  - `include_original`, `show_importance`, and `add_reverse` are optional parameters for including the original dataset, showing feature importance, and augmenting the training set by reversing, respectively.\n  - `X` is a copy of the training data, and `y` is the target variable (`Exited` column).\n\n```python\n    # initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(X)))\n    train_scores, val_scores = [], []\n    feature_importances_table = pd.DataFrame({'value': 0}, index=list(X.columns))\n    test_predictions = np.zeros((len(test)))\n```\n- **Explanation:**\n  - Initializes arrays and lists to store predictions and scores during cross-validation.\n  - `val_predictions`: An array to store validation set predictions.\n  - `train_scores` and `val_scores`: Lists to store training and validation ROC AUC scores.\n  - `feature_importances_table`: A DataFrame to store feature importance values (initialized with zeros).\n  - `test_predictions`: An array to store final test set predictions.\n\n```python\n    # training model, predicting prognosis probability, and evaluating metrics\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n```\n- **Explanation:**\n  - Initiates a loop for each fold in the cross-validation.\n  - `fold`: Current fold index.\n  - `train_idx` and `val_idx`: Indices for the training and validation sets for the current fold.\n\n```python\n        model = clone(estimator)\n```\n- **Explanation:**\n  - Creates a clone of the original machine learning model (`estimator`) for the current fold.\n\n```python\n        # define train set\n        X_train = X.iloc[train_idx].reset_index(drop=True)\n        y_train = y.iloc[train_idx].reset_index(drop=True)\n```\n- **Explanation:**\n  - Defines the training set using the current fold indices and resets the index for consistency.\n\n```python\n        # define validation set\n        X_val = X.iloc[val_idx].reset_index(drop=True)\n        y_val = y.iloc[val_idx].reset_index(drop=True)\n```\n- **Explanation:**\n  - Defines the validation set using the current fold indices and resets the index for consistency.\n\n```python\n        if include_original:\n            X_train = pd.concat([orig_train.drop('Exited', axis=1), X_train]).reset_index(drop=True)\n            y_train = pd.concat([orig_train.Exited, y_train]).reset_index(drop=True)\n```\n- **Explanation:**\n  - Optionally includes the original dataset in the training set if `include_original` is True.\n\n```python\n        if add_reverse:\n            X_train = pd.concat([X_train, X_train.iloc[::-1]]).reset_index(drop=True)\n            y_train = pd.concat([y_train, y_train.iloc[::-1]]).reset_index(drop=True)\n```\n- **Explanation:**\n  - Optionally augments the training set by adding its reversed version if `add_reverse` is True.\n\n```python\n        # train model\n        model.fit(X_train, y_train)\n```\n- **Explanation:**\n  - Trains the machine learning model on the defined training set.\n\n```python\n        # make predictions\n        train_preds = model.predict_proba(X_train)[:, 1]\n        val_preds = model.predict_proba(X_val)[:, 1]\n```\n- **Explanation:**\n  - Generates predictions (probability of positive class) for the training and validation sets.\n\n```python\n        val_predictions[val_idx] += val_preds\n        test_predictions += model.predict_proba(test)[:, 1] / cv.get_n_splits()\n```\n- **Explanation:**\n  - Accumulates the validation set predictions and averages the test set predictions over folds.\n\n```python\n        if show_importance:\n            feature_importances_table['value'] += permutation_importance(model, X_val, y_val, random_state=seed,\n                                                                         scoring=make_scorer(roc_auc_score,\n                                                                                             needs_proba=True),\n                                                                         n_repeats=5).importances_mean / cv.get_n_splits()\n```\n- **Explanation:**\n  - Optionally calculates feature importances using permutation importance if `show_importance` is True.\n\n```python\n        # evaluate model for a fold\n        train_score = roc_auc_score(y_train, train_preds)\n        val_score = roc_auc_score(y_val, val_preds)\n```\n- **Explanation:**\n  - Evaluates the ROC AUC scores for the training and validation sets.\n\n```python\n        # append model score for a fold to list\n        train_scores.append(train_score)\n        val_scores.append(val_score)\n```\n- **Explanation:**\n  - Appends the ROC AUC scores to the corresponding lists.\n\n```python\n    if show_importance:\n        plt.figure(figsize=(20, 30))\n        plt.title(f'Features with Biggest Importance of {np.mean(val_scores):.5f} ± {np.std(val_scores):.5f} Model',\n                  size=25, weight='bold')\n        sns.barplot(feature_importances_table.sort_values('value', ascending=False).T, orient='h', palette='viridis')\n        plt.show()\n    else:\n        print(\n            f'Val Score: {np.mean(val_scores):.5f} ± {np.std(val_scores):.5f} | Train Score: {np.mean(train_scores):.5f} ± {np.std(train_scores):.5f} | {label}')\n```\n- **Explanation:**\n  - Optionally displays a bar plot of feature importances if `show_importance` is True.\n  - Prints the mean and standard deviation of ROC AUC scores for training and validation sets.\n\n```python\n    val_predictions = np.where(orig_comp_combo.Exited_y == 1, 0,\n                               np.where(orig_comp_combo.Exited_y == 0, 1, val_predictions))\n    test_predictions = np.where(orig_test_combo.Exited == 1, 0,\n                                np.where(orig_test_combo.Exited == 0, 1, test_predictions))\n```\n- **Explanation:**\n  - Adjusts the predictions for cases where the original dataset has target values (`Exited_y` and `Exited`) to avoid leakage.\n\n```python\n    return val_scores, val_predictions, test_predictions\n```\n- **Explanation:**\n  - Returns the validation scores, predictions, and test predictions for analysis.\n\n### Overall Purpose:\nThis function performs a robust cross-validation for a given machine learning model, providing evaluation scores, predictions, and optional feature importance analysis. It incorporates various parameters for flexibility in the training process. The augmented training sets, inclusion of the original dataset, and feature importance analysis contribute to a thorough evaluation of the model.","metadata":{}},{"cell_type":"markdown","source":"# **Part 7: Logistic Regression Model**\n\n##### Model Pipeline:\n```python\nLog = make_pipeline(\n    SalaryRounder,\n    AgeRounder,\n    FeatureGenerator,\n    Vectorizer(cols=['Surname', 'AllCat', 'EstimatedSalary', 'CreditScore'], max_features=500, n_components=4),\n    CatBoostEncoder(cols=cat_features + [f'SurnameSVD{i}' for i in range(4)]),\n    StandardScaler(),\n    LogisticRegression(random_state=seed, max_iter=1000000000)\n)\n```\n\n##### Explanation:\n1. **SalaryRounder, AgeRounder, FeatureGenerator:**\n   - `SalaryRounder`: Rounds the 'EstimatedSalary' feature.\n   - `AgeRounder`: Rounds the 'Age' feature.\n   - `FeatureGenerator`: Generates additional features based on the existing ones, such as interaction features.\n\n2. **Vectorizer:**\n   - Utilizes TF-IDF Vectorization and Truncated Singular Value Decomposition (SVD) to create embeddings for categorical columns.\n   - Columns like 'Surname', 'AllCat', 'EstimatedSalary', and 'CreditScore' are vectorized with a limit of 500 features and 4 components.\n\n3. **CatBoostEncoder:**\n   - Applies CatBoost encoding to categorical features and the newly created SVD features.\n   - CatBoost encoding is a method to encode categorical features based on the target variable.\n\n4. **StandardScaler:**\n   - Standardizes the numerical features to have a mean of 0 and a standard deviation of 1.\n   - Ensures all features contribute equally to the logistic regression model.\n\n5. **LogisticRegression:**\n   - Implements logistic regression as the final classifier.\n   - Uses the standard logistic regression algorithm with a large number of iterations (`max_iter=1000000000`) to ensure convergence.\n\n##### Cross-validation and Training:\n```python\n_, oof_list['Log'], predict_list['Log'] = cross_val_score(Log)\n```\n- Calls the `cross_val_score` function to perform cross-validation for the Logistic Regression model.\n- Obtains the validation scores (`oof_list['Log']`), validation predictions, and test predictions for analysis.\n\n##### Overall Purpose:\nThis logistic regression model is part of a pipeline that includes feature engineering, vectorization, encoding, scaling, and logistic regression training. The pipeline is designed for robust performance on a binary classification task. The feature engineering and encoding steps aim to capture meaningful patterns in the data, while logistic regression serves as the final classifier. The model is evaluated using cross-validation to assess its generalization performance.\n\n**Explanation:**\n\nSalaryRounder, AgeRounder, FeatureGenerator: These are preprocessing steps to round specific columns.\nVectorizer: It converts selected categorical columns into numerical vectors using TF-IDF and dimensionality reduction with TruncatedSVD.\nCatBoostEncoder: Encodes categorical features using CatBoost encoding.\nStandardScaler: Standardizes the feature values.\nLogisticRegression: Implements logistic regression for binary classification.","metadata":{}},{"cell_type":"markdown","source":"# **Part 8: TensorFlow Model**\n\n##### Model Definition:\n```python\nclass TensorFlower(BaseEstimator, ClassifierMixin):\n\n    def fit(self, x, y):\n        # Model Architecture\n        inputs = tf.keras.Input((x.shape[1]))\n        inputs_norm = tf.keras.layers.BatchNormalization()(inputs)\n\n        z = tf.keras.layers.Dense(32)(inputs_norm)\n        z = tf.keras.layers.BatchNormalization()(z)\n        z = tf.keras.layers.LeakyReLU()(z)\n\n        z = tf.keras.layers.Dense(64)(z)\n        z = tf.keras.layers.BatchNormalization()(z)\n        z = tf.keras.layers.LeakyReLU()(z)\n\n        z = tf.keras.layers.Dense(16)(z)\n        z = tf.keras.layers.BatchNormalization()(z)\n        z = tf.keras.layers.LeakyReLU()(z)\n\n        z = tf.keras.layers.Dense(4)(z)\n        z = tf.keras.layers.BatchNormalization()(z)\n        z = tf.keras.layers.LeakyReLU()(z)\n\n        z = tf.keras.layers.Dense(1)(z)\n        z = tf.keras.layers.BatchNormalization()(z)\n        outputs = tf.keras.activations.sigmoid(z)\n\n        # Model Compilation\n        self.model = tf.keras.Model(inputs, outputs)\n        self.model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.AdamW(1e-4))\n\n        # Model Training\n        self.model.fit(x.to_numpy(), y, epochs=10, verbose=0)\n        self.classes_ = np.unique(y)\n\n        return self\n\n    def predict_proba(self, x):\n        predictions = np.zeros((len(x), 2))\n        predictions[:, 1] = self.model.predict(x, verbose=0)[:, 0]\n        predictions[:, 0] = 1 - predictions[:, 1]\n        return predictions\n\n    def predict(self, x):\n        return np.argmax(self.predict_proba(x), axis=1)\n```\n\n##### Model Architecture:\n1. **Input Layer:**\n   - `inputs = tf.keras.Input((x.shape[1]))`: Defines the input layer with the number of features in the input data (`x`).\n\n2. **Normalization Layer:**\n   - `inputs_norm = tf.keras.layers.BatchNormalization()(inputs)`: Applies batch normalization to normalize the input features.\n\n3. **Dense Layers with Leaky ReLU Activation:**\n   - Several densely connected layers (`Dense`) with different numbers of units and batch normalization.\n   - Each dense layer is followed by a Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n4. **Output Layer:**\n   - `z = tf.keras.layers.Dense(1)(z)`: Final dense layer with a single unit, representing the binary classification output.\n   - Batch normalization and a sigmoid activation function are applied to produce probabilities.\n\n##### Model Compilation and Training:\n- **Compilation:**\n  - `self.model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.AdamW(1e-4))`: Compiles the model with binary cross-entropy loss and the AdamW optimizer.\n\n- **Training:**\n  - `self.model.fit(x.to_numpy(), y, epochs=10, verbose=0)`: Trains the model on the input data (`x`) and labels (`y`) for 10 epochs.\n\n##### Prediction Functions:\n1. **Predict Probabilities:**\n   - `def predict_proba(self, x)`: Returns the predicted probabilities for each class in a 2D array.\n\n2. **Predict Class Labels:**\n   - `def predict(self, x)`: Returns the predicted class labels by selecting the class with the highest probability.\n\n##### Model Pipeline:\n```python\nTensorFlowey = make_pipeline(\n    SalaryRounder,\n    AgeRounder,\n    FeatureGenerator,\n    CatBoostEncoder(cols=cat_features),\n    TensorFlower()\n)\n```\n\n- Incorporates the `TensorFlower` model into a scikit-learn pipeline.\n- Preprocesses features using the `SalaryRounder`, `AgeRounder`, and `FeatureGenerator`.\n- Encodes categorical features using CatBoostEncoder.\n- Applies the TensorFlow model (`TensorFlower`) for training and prediction.\n\n##### Cross-validation and Training:\n```python\n_, oof_list['TF'], predict_list['TF'] = cross_val_score(TensorFlowey)\n```\n- Calls the `cross_val_score` function to perform cross-validation for the TensorFlow model.\n- Obtains the validation scores (`oof_list['TF']`), validation predictions, and test predictions for analysis.\n\nThis TensorFlow model is a simple feedforward neural network designed for binary classification, and it's integrated into a scikit-learn compatible pipeline for ease of use and integration with other models.\n\n**Explanation:**\n\nTensorFlower class: Custom TensorFlow model using Keras Sequential API with BatchNormalization and Dense layers.\nTensorFlowey: It is a pipeline that includes preprocessing and the custom TensorFlow model.\ncross_val_score: Cross-validates and evaluates the TensorFlow model.","metadata":{}},{"cell_type":"markdown","source":"# **Part 9: XGBoost Model**\n\n##### Objective Function for Optuna:\n```python\ndef xgb_objective(trial):\n    # Hyperparameter search space\n    params = {\n        'eta': trial.suggest_float('eta', .001, .3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 2, 30),\n        'subsample': trial.suggest_float('subsample', .5, 1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', .1, 1),\n        'min_child_weight': trial.suggest_float('min_child_weight', .1, 20, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', .01, 20, log=True),\n        'reg_alpha': trial.suggest_float('reg_alpha', .01, 10, log=True),\n        'n_estimators': 1000,\n        'random_state': seed,\n        'tree_method': 'hist',\n    }\n\n    # XGBoost model within an Optuna pipeline\n    optuna_model = make_pipeline(\n        SalaryRounder,\n        AgeRounder,\n        FeatureGenerator,\n        Vectorizer(cols=['Surname', 'AllCat', 'EstimatedSalary', 'CustomerId'], max_features=1000, n_components=3),\n        CatBoostEncoder(cols=['CustomerId', 'Surname', 'EstimatedSalary', 'AllCat', 'CreditScore']),\n        MEstimateEncoder(cols=['Geography', 'Gender']),\n        XGBClassifier(**params)\n    )\n\n    # Cross-validation and scoring\n    optuna_score, _, _ = cross_val_score(optuna_model)\n\n    return np.mean(optuna_score)\n```\n\n- Defines an objective function for Optuna hyperparameter optimization.\n- Searches for optimal hyperparameters for XGBoost using Optuna.\n\n##### Optuna Study:\n```python\nxgb_study = optuna.create_study(direction='maximize')\n\n# xgb_study.optimize(xgb_objective, 50)\nxgb_params = {'eta': 0.04007938900538817, 'max_depth': 5, 'subsample': 0.8858539721226424,\n              'colsample_bytree': 0.41689519430449395, 'min_child_weight': 0.4225662401139526,\n              'reg_lambda': 1.7610231110037127, 'reg_alpha': 1.993860687732973}\n```\n\n- Creates an Optuna study for hyperparameter optimization with a maximization objective.\n- Optionally, performs hyperparameter optimization (commented out in the code).\n\n##### XGBoost Model Pipeline:\n```python\nXGB = make_pipeline(\n    SalaryRounder,\n    AgeRounder,\n    FeatureGenerator,\n    Vectorizer(cols=['Surname', 'AllCat', 'EstimatedSalary', 'CustomerId'], max_features=1000, n_components=3),\n    CatBoostEncoder(cols=['CustomerId', 'Surname', 'EstimatedSalary', 'AllCat', 'CreditScore']),\n    MEstimateEncoder(cols=['Geography', 'Gender']),\n    XGBClassifier(**xgb_params, random_state=seed, tree_method='hist', n_estimators=1000)\n)\n```\n\n- Defines the XGBoost model pipeline with fixed hyperparameters or those optimized through Optuna.\n- Preprocesses features, encodes categorical features, and applies the XGBoost classifier.\n\n##### Cross-validation and Training:\n```python\n_, oof_list['XGB'], predict_list['XGB'] = cross_val_score(XGB, show_importance=False)\n```\n\n- Calls the `cross_val_score` function to perform cross-validation for the XGBoost model.\n- Obtains the validation scores (`oof_list['XGB']`), validation predictions, and test predictions for analysis.\n\nThis XGBoost model leverages Optuna for hyperparameter optimization and is integrated into a scikit-learn compatible pipeline for consistent preprocessing and model training. The hyperparameters can be fixed or optimized based on the use of the Optuna study.\n\n**Explanation:**\n\nxgb_objective: This function defines the optimization objective for Optuna to find the best hyperparameters for XGBoost.\nXGBClassifier: It is a gradient boosting algorithm using decision trees as base learners.","metadata":{}},{"cell_type":"markdown","source":"# **Part 10: LightGBM Model**\n\n##### Objective Function for Optuna:\n```python\ndef lgb_objective(trial):\n    # Hyperparameter search space\n    params = {\n        'learning_rate': trial.suggest_float('learning_rate', .001, .1, log=True),\n        'max_depth': trial.suggest_int('max_depth', 2, 20),\n        'subsample': trial.suggest_float('subsample', .5, 1),\n        'min_child_weight': trial.suggest_float('min_child_weight', .1, 15, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', .1, 20, log=True),\n        'reg_alpha': trial.suggest_float('reg_alpha', .1, 10, log=True),\n        'n_estimators': 1000,\n        'random_state': seed,\n        # 'boosting_type' : 'dart',\n    }\n\n    # LightGBM model within an Optuna pipeline\n    optuna_model = make_pipeline(\n        SalaryRounder,\n        AgeRounder,\n        FeatureGenerator,\n        Vectorizer(cols=['Surname', 'AllCat'], max_features=1000, n_components=3),\n        CatBoostEncoder(cols=['Surname', 'AllCat', 'CreditScore', 'Age']),\n        MEstimateEncoder(cols=['Geography', 'Gender', 'NumOfProducts']),\n        StandardScaler(),\n        LGBMClassifier(**params)\n    )\n\n    # Cross-validation and scoring\n    optuna_score, _, _ = cross_val_score(optuna_model)\n\n    return np.mean(optuna_score)\n```\n\n- Defines an objective function for Optuna hyperparameter optimization.\n- Searches for optimal hyperparameters for LightGBM using Optuna.\n\n##### Optuna Study:\n```python\nlgb_study = optuna.create_study(direction='maximize')\n\n# lgb_study.optimize(lgb_objective, 100)\nlgb_params = {'learning_rate': 0.01864960338160943, 'max_depth': 9, 'subsample': 0.6876252164703066,\n              'min_child_weight': 0.8117588782708633, 'reg_lambda': 6.479178739677389, 'reg_alpha': 3.2952573115561234}\n```\n\n- Creates an Optuna study for hyperparameter optimization with a maximization objective.\n- Optionally, performs hyperparameter optimization (commented out in the code).\n\n##### LightGBM Model Pipeline:\n```python\nLGB = make_pipeline(\n    SalaryRounder,\n    AgeRounder,\n    FeatureGenerator,\n    Vectorizer(cols=['Surname', 'AllCat'], max_features=1000, n_components=3),\n    CatBoostEncoder(cols=['Surname', 'AllCat', 'CreditScore', 'Age']),\n    MEstimateEncoder(cols=['Geography', 'Gender', 'NumOfProducts']),\n    StandardScaler(),\n    LGBMClassifier(**lgb_params, random_state=seed, n_estimators=1000)\n)\n```\n\n- Defines the LightGBM model pipeline with fixed hyperparameters or those optimized through Optuna.\n- Preprocesses features, encodes categorical features, and applies the LightGBM classifier.\n\n##### Cross-validation and Training:\n```python\n_, oof_list['LGB'], predict_list['LGB'] = cross_val_score(LGB, show_importance=False)\n```\n\n- Calls the `cross_val_score` function to perform cross-validation for the LightGBM model.\n- Obtains the validation scores (`oof_list['LGB']`), validation predictions, and test predictions for analysis.\n\nThis LightGBM model leverages Optuna for hyperparameter optimization and is integrated into a scikit-learn compatible pipeline for consistent preprocessing and model training. The hyperparameters can be fixed or optimized based on the use of the Optuna study.\n\n**Explanation:**\n\nlgb_objective: This function defines the optimization objective for Optuna to find the best hyperparameters for LightGBM.\nLGBMClassifier: It is a gradient boosting framework using tree-based learning algorithms.","metadata":{}},{"cell_type":"markdown","source":"# **Part 11: CatBoost Models**\n\nIn this section, three CatBoost models (`CB`, `CB_Bayes`, `CB_Bernoulli`) are constructed within a scikit-learn `Pipeline`. These models are then evaluated using cross-validation.\n\n##### 1. **Basic CatBoost Model (`CB`):**\n```python\nCB = make_pipeline(\n    SalaryRounder,\n    AgeRounder,\n    FeatureGenerator,\n    Vectorizer(cols=['Surname', 'AllCat'], max_features=1000, n_components=4),\n    SVDRounder,\n    CatBoostClassifier(random_state=seed, verbose=0, cat_features=cat_features + [f'SurnameSVD{i}' for i in range(4)],\n                       has_time=True)\n)\n\n_, oof_list['CB'], predict_list['CB'] = cross_val_score(CB, show_importance=False)\n```\n\n   - **Pipeline Setup:**\n     - A scikit-learn `Pipeline` is created (`CB`) to organize a sequence of preprocessing and modeling steps.\n     - Preprocessing steps include rounding specific numerical features (`SalaryRounder`, `AgeRounder`), generating new features (`FeatureGenerator`), vectorizing text-based features (`Vectorizer`), applying truncated SVD (`SVDRounder`), and finally using a CatBoostClassifier.\n\n   - **Model Configuration:**\n     - The CatBoostClassifier is configured with specific parameters:\n       - `random_state`: Ensures reproducibility.\n       - `verbose=0`: Suppresses CatBoost's output during training.\n       - `cat_features`: Specifies categorical features, including additional features generated by truncated SVD.\n       - `has_time=True`: Indicates the dataset has a time component.\n\n   - **Cross-Validation:**\n     - The `cross_val_score` function is used to perform cross-validation on this CatBoost model.\n     - The results are stored in the `oof_list['CB']` (out-of-fold predictions) and `predict_list['CB']` (test predictions).\n\n##### 2. **CatBoost with Bayesian Bootstrap (`CB_Bayes`):**\n```python\nCB_Bayes = make_pipeline(\n    SalaryRounder,\n    AgeRounder,\n    FeatureGenerator,\n    Vectorizer(cols=['Surname', 'AllCat'], max_features=1000, n_components=4),\n    SVDRounder,\n    CatBoostClassifier(random_state=seed, verbose=0, cat_features=cat_features + [f'SurnameSVD{i}' for i in range(4)],\n                       bootstrap_type='Bayesian', has_time=True)\n)\n\n_, oof_list['CB_Bayes'], predict_list['CB_Bayes'] = cross_val_score(CB_Bayes, show_importance=False)\n```\n\n   - **Model Variation:**\n     - The second CatBoost model (`CB_Bayes`) is similar to the basic model but introduces a change in the bootstrap sampling type.\n     - `bootstrap_type='Bayesian'`: Utilizes Bayesian bootstrap sampling.\n\n   - **Cross-Validation:**\n     - The model is evaluated using cross-validation, and the results are stored in `oof_list['CB_Bayes']` and `predict_list['CB_Bayes']`.\n\n##### 3. **CatBoost with Bernoulli Bootstrap (`CB_Bernoulli`):**\n```python\nCB_Bernoulli = make_pipeline(\n    SalaryRounder,\n    AgeRounder,\n    FeatureGenerator,\n    Vectorizer(cols=['Surname', 'AllCat'], max_features=1000, n_components=4),\n    SVDRounder,\n    CatBoostClassifier(random_state=seed, verbose=0, cat_features=cat_features + [f'SurnameSVD{i}' for i in range(4)],\n                       bootstrap_type='Bernoulli', has_time=True)\n)\n\n_, oof_list['CB_Bernoulli'], predict_list['CB_Bernoulli'] = cross_val_score(CB_Bernoulli, show_importance=False)\n```\n\n   - **Model Variation:**\n     - The third CatBoost model (`CB_Bernoulli`) introduces another change in the bootstrap sampling type.\n     - `bootstrap_type='Bernoulli'`: Utilizes Bernoulli bootstrap sampling.\n\n   - **Cross-Validation:**\n     - The model is evaluated using cross-validation, and the results are stored in `oof_list['CB_Bernoulli']` and `predict_list['CB_Bernoulli']`.\n\nThese models represent variations of CatBoost with different bootstrap sampling strategies, allowing for flexibility and exploration of model performance under different conditions. The cross-validation results provide insights into the models' predictive capabilities.","metadata":{}},{"cell_type":"markdown","source":"# **Part 12: Voting Ensemble Detailed Explanation**\n\n```python\n# Voting Ensemble\nweights = RidgeClassifier(random_state=seed).fit(oof_list, train.Exited).coef_[0]\nweights /= weights.sum()\npd.DataFrame(weights, index=list(oof_list), columns=['weight per model'])\n\n# _, ensemble_oof, predictions = cross_val_score(voter, show_importance = False)\nprint(f'Score: {(roc_auc_score(train.Exited, oof_list.to_numpy() @ weights)):.5f}')\npredictions = predict_list.to_numpy() @ weights\n```\n\n**1. RidgeClassifier for Weight Learning:**\n   - A `RidgeClassifier` is employed to learn the optimal weights for combining predictions from different models.\n   - The classifier is trained on the out-of-fold (oof) predictions (`oof_list`) from various models, using the actual target values (`train.Exited`).\n   - The `random_state=seed` ensures reproducibility.\n\n**2. Normalization of Weights:**\n   - The coefficients obtained from the trained `RidgeClassifier` represent the weights assigned to each model in the ensemble.\n   - These weights are normalized to ensure that they sum up to 1, making them interpretable as proportions.\n\n**3. Display Weights:**\n   - The learned weights are presented in a DataFrame to provide transparency on how much influence each model has in the ensemble.\n\n**4. Ensemble Evaluation:**\n   - The ROC AUC score is calculated for the ensemble using the learned weights (`oof_list.to_numpy() @ weights`).\n   - This score is printed to assess the overall performance of the ensemble.\n\n**5. Final Test Predictions:**\n   - The final predictions for the test set are obtained by multiplying the predictions from individual models (`predict_list.to_numpy()`) with their respective weights.\n   - The matrix multiplication (`@`) and summation produce the ensemble's weighted prediction for each sample in the test set.\n\n**Note:**\n- The use of a RidgeClassifier for weight learning implies a linear combination of models, providing interpretable and stable results.\n- The ensemble leverages the diversity captured by individual models, assigning different weights based on their performance on the training data.","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"background-color: lightgreen; font-size: 40px; text-align: center;\"> Detailed explanation of each model </div>\n\n### 1. **Linear Models:**\n\n#### Logistic Regression:\n   - **Explanation:**\n     - Logistic Regression is a classic linear model used for binary classification problems.\n     - It models the probability that an instance belongs to a particular class.\n     - It's based on the logistic function, which maps any real-valued number into a range between 0 and 1.\n\n#### RidgeClassifier:\n   - **Explanation:**\n     - RidgeClassifier is a linear model that uses Ridge Regression for classification.\n     - Ridge Regression is an extension of linear regression with regularization to prevent overfitting.\n     - It introduces a penalty term (L2 regularization) to the linear regression loss function.\n\n### 2. **Tree-Based Models:**\n\n#### XGBoostClassifier:\n   - **Explanation:**\n     - XGBoost is an ensemble learning method known for its speed and performance.\n     - It builds multiple decision trees and combines them to improve accuracy.\n     - It uses a gradient boosting framework and incorporates regularization techniques.\n\n#### LightGBMClassifier:\n   - **Explanation:**\n     - LightGBM is a gradient boosting framework developed by Microsoft.\n     - It is designed for distributed and efficient training.\n     - LightGBM uses a histogram-based approach for tree building, leading to faster training times.\n\n#### CatBoostClassifier:\n   - **Explanation:**\n     - CatBoost is a gradient boosting library developed by Yandex, designed for categorical features.\n     - It handles categorical features efficiently without the need for one-hot encoding.\n     - It includes built-in support for handling missing data.\n\n### 3. **Neural Network Model:**\n\n#### TensorFlow (Custom Neural Network):\n   - **Explanation:**\n     - TensorFlow is an open-source machine learning framework developed by Google.\n     - A custom neural network model is built using TensorFlow for this project.\n     - It consists of multiple layers, including dense layers with batch normalization and leaky ReLU activation functions.\n     - The model is trained using binary crossentropy loss and the AdamW optimizer.\n\n### 4. **Ensemble Models:**\n\n#### Voting Ensemble:\n   - **Explanation:**\n     - A Voting Ensemble combines predictions from multiple models.\n     - Each model's prediction is weighted based on its performance.\n     - The weights are learned using RidgeClassifier on out-of-fold predictions.\n\n### Overall:\n   - **Objective:**\n     - The goal is to predict the target variable \"Exited,\" indicating whether a customer will exit the bank or not.\n   - **Diversity:**\n     - Multiple types of models are tested to capture different patterns and relationships in the data.\n   - **Evaluation:**\n     - Model performance is assessed using cross-validation, and metrics like AUC-ROC score are used to evaluate the models' effectiveness.\n\nThis comprehensive approach with various model types aims to improve the predictive performance and robustness of the overall solution.","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"background-color: lightblue; font-size: 40px; text-align: center;\"> Conclusion: Customer Churn Prediction Project </div>\n\nIn this comprehensive customer churn prediction project, our objective was to construct an effective predictive model within a banking context. The project unfolded across three pivotal phases: data exploration, feature engineering, and model evaluation.\n\n**1. Data Exploration:**\n   - Rigorously examined the dataset, encompassing both training and testing data, to obtain a profound understanding of its features.\n   - Explored customer demographics, financial indicators, and transactional behavior, extracting valuable insights.\n   - Addressed data cleanliness issues by investigating data types, handling missing values, and analyzing variable distributions.\n\n**2. Feature Engineering:**\n   - Implemented diverse techniques to augment model predictability and enhance generalization capabilities.\n   - Introduced innovative features such as 'IsActive_by_CreditCard,' 'Products_Per_Tenure,' and 'ZeroBalance' to capture critical patterns.\n   - Employed feature transformations, including TF-IDF vectorization and singular value decomposition (SVD) on selected categorical variables ('Surname,' 'AllCat,' 'EstimatedSalary,' 'CreditScore'), to uncover latent patterns.\n\n**3. Model Evaluation:**\n   - Evaluated a variety of machine learning models, including Logistic Regression, a custom TensorFlow model ('TensorFlower'), and an XGBoost classifier.\n   - Applied cross-validation techniques to ensure robust performance assessment.\n   - Achieved the highest validation score of 0.90492 using the Voting Ensemble with Ridge Classifier weights.\n\n**Key Findings:**\n   - The logistic regression model established a strong baseline performance, achieving a validation score of approximately 88.42%.\n   - The custom TensorFlow model ('TensorFlower') demonstrated promising results, surpassing the logistic regression model with a validation score of around 89.23%.\n   - The Voting Ensemble with Ridge Classifier weights emerged as the top-performing model, achieving the highest validation score of 0.90492.\n\n**Recommendations and Future Work:**\n   - Based on our analysis, we recommend deploying the Voting Ensemble model with Ridge Classifier weights for customer churn prediction, given its outstanding performance.\n   - Continuous monitoring and periodic recalibration will be crucial to ensure the model's sustained effectiveness in dynamic business environments.\n   - Future work may explore additional ensemble techniques and feature engineering strategies to further refine model performance.\n\n**In summary, this project culminates in a robust predictive model that empowers proactive identification of customers at risk of churning, providing a valuable tool for targeted retention strategies and fostering a customer-centric approach within the banking domain.**","metadata":{}}]}